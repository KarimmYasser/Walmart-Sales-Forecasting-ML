# ðŸ›’ Walmart Sales Forecasting Project

**AI & Data Science Track - Round 2**  
**Project Type:** Time Series Forecasting  
**Dataset:** Walmart Recruiting Store Sales Forecasting (Kaggle)  
**Status:** Milestone 1 Complete (85%) - Ready for Model Development

---

## ðŸ“‹ Project Overview

This project develops a machine learning system to forecast weekly sales for Walmart stores across 45 locations and 99 departments. The goal is to predict future sales using historical data, store characteristics, external factors, and promotional activities.

### ðŸŽ¯ Business Objectives

- **Inventory Optimization**: Prevent stockouts and overstocking
- **Staff Scheduling**: Allocate resources based on predicted demand
- **Marketing Planning**: Time promotions for maximum impact
- **Financial Forecasting**: Accurate revenue projections

### ðŸ“Š Success Metrics

- **MAE (Mean Absolute Error)**: < $3,000 per week
- **RMSE (Root Mean Square Error)**: < $5,000 per week
- **MAPE (Mean Absolute Percentage Error)**: < 15%
- **Baseline Improvement**: Beat naive forecasting by 25%+

---

## ðŸ“ Dataset Summary

| Dataset | Records | Columns | Date Range | Description |
|---------|---------|---------|------------|-------------|
| **Training** | 421,570 | 49 | 2010-02-05 to 2012-10-26 | Historical sales with target variable |
| **Test** | 115,064 | 48 | 2012-11-02 to 2013-07-26 | Future period for predictions |
| **Stores** | 45 | 3 | - | Store metadata (Type A/B/C, Size) |
| **Features** | 8,190 | 12 | - | External factors (Temperature, CPI, etc.) |

### Key Features

**Original (10):** Size, Temperature, Fuel_Price, MarkDown1-5, CPI, Unemployment  
**Engineered (39):** Time features (20), Lag features (7), Encoded categories (3), Promotion flags (5), Holiday (1)  
**Total Features:** 49 (train), 48 (test)

---

## ðŸŽ¯ Project Milestones

### âœ… Milestone 1: Data Collection, Exploration & Preprocessing (85% Complete)

**Completed:**
- âœ… **Task 1: Data Collection** - Acquired and merged 4 datasets
- âœ… **Task 2: Data Exploration** - EDA with 10 visualizations, missing value analysis
- âœ… **Task 3: Feature Engineering** - 20 time features, 7 lag features, encoding, normalization
- âœ… **Task 4: EDA** - Comprehensive analysis with actionable insights

**Deliverables:**
- âœ… Cleaned Dataset (100%): `processed_data/Final/train_final.csv` & `test_final.csv`
- âœ… Interactive Visualizations (100%): 10 professional visualizations
- ðŸŸ¡ EDA Report (85%): Analysis complete, formal writeup pending

### â³ Milestone 2: Model Development (Next Phase)

- Build baseline models (naive forecasting)
- Train advanced models (Random Forest, XGBoost, LSTM)
- Hyperparameter tuning
- Model evaluation and comparison

### â³ Milestone 3: Model Evaluation & Selection

- Cross-validation for time series
- Performance metrics analysis
- Best model selection
- Error analysis

### â³ Milestone 4: Deployment & Monitoring

- Deploy model (Flask/Streamlit)
- Create prediction API
- Set up monitoring for drift
- Production testing

### â³ Milestone 5: Final Documentation & Presentation

- Final report
- Presentation slides
- Code documentation
- Deployment guide

---

## ðŸ”‘ Key Insights from EDA

### ðŸ“ˆ Seasonality (Critical!)
- **Q4 sales are 35-40% higher than Q1** - Strong holiday surge
- November and December are peak months
- Models must capture seasonal patterns

### ðŸŽ‰ Holiday Impact
- **+11.6% sales lift** during holiday weeks
- Consistent across all store types
- IsHoliday is a strong predictor

### ðŸ’° Promotion Effectiveness
- **All markdowns increase sales** (positive ROI)
- MarkDown5: +22.1% lift (most effective)
- MarkDown1: +18.9% lift (second best)
- Promotion features are valuable predictors

### ðŸª Store Types
- **Type A (Large)**: 55% of sales, highest variance
- **Type B (Medium)**: 30% of sales, stable performance
- **Type C (Small)**: 15% of sales, most consistent
- Store type segmentation is critical

### ðŸ“Š External Factors
- **Unemployment**: Strongest correlation (-0.128)
- Temperature, Fuel Price: Minimal impact
- CPI: Moderate correlation

### ðŸŽ¯ Department Concentration
- **Top 10 departments = 66% of total sales**
- Power law distribution suggests focused forecasting

---

## ðŸ› ï¸ Technical Stack

**Languages & Libraries:**
- Python 3.12
- pandas, numpy (data manipulation)
- matplotlib, seaborn (visualization)
- scikit-learn (preprocessing, modeling)

**Data Processing:**
- Openpyxl (Excel files)
- Manual Z-score normalization
- One-Hot encoding for categories

**Models (Planned):**
- Random Forest Regressor
- XGBoost
- LSTM (for time series)
- ARIMA/SARIMA

---

## ðŸ“‚ Project Structure

```
Depi_project_Data-science/
â”‚
â”œâ”€â”€ datasets/                          # Raw data
â”‚   â””â”€â”€ walmart-recruiting-store-sales-forecasting/
â”‚       â”œâ”€â”€ train.csv
â”‚       â”œâ”€â”€ test.csv
â”‚       â”œâ”€â”€ stores.csv
â”‚       â””â”€â”€ features.xlsx
â”‚
â”œâ”€â”€ processed_data/                    # Processed data pipeline
â”‚   â”œâ”€â”€ Stage1.1/                      # Merged data
â”‚   â”œâ”€â”€ Stage1.2/                      # Missing values handled
â”‚   â”œâ”€â”€ Stage1.3.1/                    # Time features added
â”‚   â”œâ”€â”€ Stage1.3.2/                    # Lag features added
â”‚   â”œâ”€â”€ Stage1.3.3/                    # Categorical encoded
â”‚   â””â”€â”€ Final/                         # â­ READY FOR MODELING
â”‚       â”œâ”€â”€ train_final.csv            # 421,570 Ã— 49
â”‚       â”œâ”€â”€ test_final.csv             # 115,064 Ã— 48
â”‚       â””â”€â”€ normalization_params.json  # For production
â”‚
â”œâ”€â”€ visualizations/                    # EDA outputs
â”‚   â””â”€â”€ Stage1.4/
â”‚       â”œâ”€â”€ 01_overall_sales_trend.png
â”‚       â”œâ”€â”€ 05_holiday_impact.png
â”‚       â””â”€â”€ ... (10 visualizations)
â”‚
â”œâ”€â”€ Scripts/                           # Milestone 1 scripts
â”‚   â”œâ”€â”€ step_1_1_data_loading_merging.py
â”‚   â”œâ”€â”€ step_1_2_missing_values.py
â”‚   â”œâ”€â”€ step_1_4_eda_analysis.py
â”‚   â”œâ”€â”€ step_1_3_1_time_features.py
â”‚   â”œâ”€â”€ step_1_3_2_lag_features.py
â”‚   â”œâ”€â”€ step_1_3_3_encode_categorical.py
â”‚   â””â”€â”€ step_1_3_4_normalize_features_final.py
â”‚
â”œâ”€â”€ DOCUMENTATION.md                   # ðŸ“˜ Complete project documentation
â”œâ”€â”€ README.md                          # ðŸ“„ This file
â””â”€â”€ main.py                           # Initial exploration script
```

---

## ðŸš€ Quick Start

### Prerequisites
```bash
pip install pandas numpy matplotlib seaborn openpyxl
```

### Running the Pipeline

**Step 1: Data Loading**
```bash
python step_1_1_data_loading_merging.py
```

**Step 2: Handle Missing Values**
```bash
python step_1_2_missing_values.py
```

**Step 3: Feature Engineering**
```bash
python step_1_3_1_time_features.py
python step_1_3_2_lag_features.py
python step_1_3_3_encode_categorical.py
python step_1_3_4_normalize_features_final.py
```

**Step 4: Exploratory Data Analysis**
```bash
python step_1_4_eda_analysis.py
```

### Loading Final Data
```python
import pandas as pd

# Load modeling-ready data
train = pd.read_csv('processed_data/Final/train_final.csv')
test = pd.read_csv('processed_data/Final/test_final.csv')

print(f"Train: {train.shape}")  # (421570, 49)
print(f"Test: {test.shape}")    # (115064, 48)

# Features are normalized, encoded, and ready for ML!
```

---

## ðŸ”„ Data Generation Pipeline

### **Option 1: Automated Pipeline (RECOMMENDED)**

Run the complete feature engineering pipeline in one command:

```bash
python feature_engineering_pipeline.py
```

**â±ï¸ Execution Time:** ~60 seconds  
**ðŸ’¾ Total Output:** ~506 MB of processed data

#### What Gets Generated:

```
processed_data/
â”‚
â”œâ”€â”€ Stage1.3.1/                    # After Time Features
â”‚   â”œâ”€â”€ train_time_features.csv    # 421,570 Ã— 40 cols (~180 MB)
â”‚   â””â”€â”€ test_time_features.csv     # 115,064 Ã— 39 cols (~48 MB)
â”‚
â”œâ”€â”€ Stage1.3.2/                    # After Lag Features
â”‚   â”œâ”€â”€ train_lag_features.csv     # 421,570 Ã— 47 cols (~210 MB)
â”‚   â””â”€â”€ test_lag_features.csv      # 115,064 Ã— 46 cols (~56 MB)
â”‚
â”œâ”€â”€ Stage1.3.3/                    # After Encoding
â”‚   â”œâ”€â”€ train_encoded.csv          # 421,570 Ã— 49 cols (~223 MB)
â”‚   â””â”€â”€ test_encoded.csv           # 115,064 Ã— 48 cols (~60 MB)
â”‚
â””â”€â”€ Final/                         # â­ FINAL OUTPUT
    â”œâ”€â”€ train_final.csv            # 421,570 Ã— 49 cols (~223 MB)
    â”œâ”€â”€ test_final.csv             # 115,064 Ã— 48 cols (~60 MB)
    â””â”€â”€ normalization_params.json  # Normalization parameters (1.5 KB)
```

#### Pipeline Stages:

**Stage 1.3.1: Time-Based Features (19 features)**
- âœ… Year, Month, Day, Quarter, DayOfWeek, WeekOfYear
- âœ… Binary: Is_Weekend, Is_Month_Start/End, Is_Quarter_Start/End, Is_Year_Start/End
- âœ… Cyclical: Month_Sin/Cos, Week_Sin/Cos, DayOfWeek_Sin/Cos
- ðŸ“Š Output: 40 columns (train) | 39 columns (test)

**Stage 1.3.2: Lag Features (7 features)**
- âœ… Sales_Lag1, Sales_Lag2, Sales_Lag4
- âœ… Sales_Rolling_Mean_4, Sales_Rolling_Mean_8
- âœ… Sales_Rolling_Std_4, Sales_Momentum
- ðŸ“Š Output: 47 columns (train) | 46 columns (test)

**Stage 1.3.3: Categorical Encoding (3 features)**
- âœ… Type â†’ Type_A, Type_B, Type_C (One-Hot Encoding)
- ðŸ“Š Output: 49 columns (train) | 48 columns (test)

**Stage 1.3.4: Numerical Normalization (17 features)**
- âœ… Z-score normalization: (X - Î¼) / Ïƒ
- âœ… Features: Size, Temperature, Fuel_Price, CPI, Unemployment, MarkDown1-5, All lag features
- ðŸ“Š Output: 49 columns (train) | 48 columns (test)

#### Console Output:

```
================================================================================
ðŸš€ FEATURE ENGINEERING PIPELINE
================================================================================

â° STEP 1.3.1: CREATING TIME-BASED FEATURES
âœ… Created 19 time-based features
ðŸ’¾ Saving Step 1.3.1 output...

ðŸ“Š STEP 1.3.2: CREATING LAG FEATURES
âœ… Created 7 lag features
ðŸ’¾ Saving Step 1.3.2 output...

ðŸ”¤ STEP 1.3.3: ENCODING CATEGORICAL VARIABLES
âœ… Encoded categorical variables
ðŸ’¾ Saving Step 1.3.3 output...

ðŸ“ STEP 1.3.4: NORMALIZING NUMERICAL FEATURES
âœ… Normalization complete!
ðŸ’¾ Saving final datasets...

ðŸŽ‰ FEATURE ENGINEERING PIPELINE COMPLETE!
ðŸš€ Ready for Model Development (Milestone 2)!
```

---

### **Option 2: Step-by-Step Execution**

If you need to run preprocessing before feature engineering or want more control:

#### Step 1: Data Loading & Merging
```bash
python step_1_1_data_loading_merging.py
```
**Output:** `processed_data/Stage1.1/train_merged.csv`, `test_merged.csv`

#### Step 2: Handle Missing Values
```bash
python step_1_2_missing_values.py
```
**Output:** `processed_data/Stage1.2/train_cleaned_step2.csv`, `test_cleaned_step2.csv`

#### Step 3: Feature Engineering (Individual Steps)
```bash
python step_1_3_1_time_features.py        # Time-based features
python step_1_3_2_lag_features.py         # Lag features
python step_1_3_3_encode_categorical.py   # Categorical encoding
python step_1_3_4_normalize_features_final.py  # Normalization
```

#### Step 4: Exploratory Data Analysis
```bash
python step_1_4_eda_analysis.py
```
**Output:** 10 visualizations in `visualizations/Stage1.4/`

---

### ðŸ“‹ Verifying Generated Data

After running the pipeline, verify your data:

```bash
# Windows PowerShell
dir processed_data\Final

# Expected Output:
# train_final.csv          223 MB
# test_final.csv            60 MB
# normalization_params.json  1.5 KB
```

**Check data integrity:**
```python
import pandas as pd

train = pd.read_csv('processed_data/Final/train_final.csv')
test = pd.read_csv('processed_data/Final/test_final.csv')

print(f"Train shape: {train.shape}")  # Should be (421570, 49)
print(f"Test shape: {test.shape}")    # Should be (115064, 48)
print(f"Missing values (train): {train.isnull().sum().sum()}")  # Should be 0
print(f"Missing values (test): {test.isnull().sum().sum()}")    # Should be 0
print(f"Duplicates (train): {train.duplicated().sum()}")        # Should be 0
```

---

### ðŸŽ¯ Which Data to Use?

| Purpose | Use This File | Reason |
|---------|---------------|---------|
| **Model Training & Testing** | `processed_data/Final/train_final.csv` & `test_final.csv` | â­ Fully processed, normalized, ready for ML |
| **Intermediate Analysis** | `processed_data/Stage1.3.X/` files | View data after specific transformation steps |
| **EDA / Visualization** | `processed_data/Stage1.2/` files | Original scale, easier to interpret |
| **Raw Data** | `datasets/walmart-recruiting-store-sales-forecasting/` | Unprocessed original datasets |

---

## ðŸ“Š Feature Engineering Pipeline

### 1. **Time-Based Features (20)**
- Basic: Year, Month, Day, Quarter, DayOfWeek, WeekOfYear
- Binary: Is_Weekend, Is_Month_Start/End, Is_Quarter_Start/End, Is_Year_Start/End
- Cyclical: Month_Sin/Cos, Week_Sin/Cos, DayOfWeek_Sin/Cos

### 2. **Lag Features (7)**
- Sales_Lag1, Sales_Lag2, Sales_Lag4 (historical sales per Store-Dept)
- Sales_Rolling_Mean_4, Sales_Rolling_Mean_8 (smoothed trends)
- Sales_Rolling_Std_4 (volatility)
- Sales_Momentum (change rate)

### 3. **Categorical Encoding (3)**
- Type_A, Type_B, Type_C (one-hot encoded store types)

### 4. **Normalization (17 features)**
- Z-Score: (x - Î¼) / Ïƒ for all continuous features
- Preserves patterns while standardizing scale

---

## ðŸ“ˆ Results & Insights

### Data Quality
- âœ… **0 missing values** (100% complete)
- âœ… **0 duplicates**
- âœ… **421,570 training examples**
- âœ… **49 engineered features**

### Patterns Discovered
- ðŸ“Š Strong seasonality (Q4 peak)
- ðŸŽ‰ Consistent holiday lift (+11.6%)
- ðŸ’° Effective promotions (up to +22% lift)
- ðŸª Clear store type differences
- ðŸ“‰ Economic indicators matter (Unemployment -0.128 correlation)

### Data Readiness
- âœ… All preprocessing complete
- âœ… Features normalized (mean=0, std=1)
- âœ… Train-test consistency maintained
- âœ… No data leakage
- âœ… Production parameters saved

---

## ðŸŽ“ Key Learnings

1. **Seasonality is Dominant**: Q4 surge must be captured by models
2. **Holidays Matter**: Simple IsHoliday flag provides +11.6% predictive power
3. **Promotions Work**: All MarkDown types increase sales
4. **Store Segmentation**: Type A/B/C behave differently
5. **Lag Features Critical**: Historical sales are strong predictors
6. **Normalization Essential**: Features had vastly different scales

---

## ðŸ“ Next Steps

### Immediate (Milestone 2)
1. **Build Baseline Model** - Naive forecasting for comparison
2. **Random Forest** - Start with ensemble method
3. **XGBoost** - Gradient boosting for accuracy
4. **LSTM** - Deep learning for time series
5. **Model Comparison** - Evaluate all models against metrics

### Future Enhancements
- Department-specific models
- Store clustering for targeted forecasting
- Promotional optimization
- Real-time prediction API
- Automated retraining pipeline

---

## ðŸ‘¥ Team

**Data Science Team**  
**Project:** AI & Data Science Track - Round 2  
**Institution:** DEPI (Digital Egypt Pioneers Initiative)

---

## ðŸ“š Documentation

For detailed implementation steps, code explanations, and complete analysis, see:
- **[DOCUMENTATION.md](DOCUMENTATION.md)** - Complete project documentation (55KB, comprehensive)

---

## ðŸ“§ Contact

For questions or collaboration:
- Project Repository: `D:\projects\Depi_project_Data-science`
- Last Updated: October 23, 2025

---

## âš–ï¸ License

This project is developed for educational purposes as part of the DEPI AI & Data Science Track.

---

**Status:** âœ… Milestone 1 Complete | ðŸš€ Ready for Model Development | ðŸ“Š Dataset: 421K training examples, 49 features
