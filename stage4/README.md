# Stage 4: MLOps, Deployment, and Monitoring

## ğŸ“‹ Overview

This stage implements MLOps practices, deploys the sales forecasting model, and sets up monitoring infrastructure for production use.

## ğŸ¯ Milestone 4 Objectives

- âœ… Implement MLOps with experiment tracking (MLflow)
- âœ… Deploy model as REST API (FastAPI)
- âœ… Create interactive dashboard (Streamlit)
- âœ… Set up model monitoring and drift detection
- âœ… Containerize application with Docker

---

## ğŸ“ Directory Structure

```
stage4/
â”‚
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ docker-compose.yml             # Multi-container orchestration
â”œâ”€â”€ Dockerfile                     # Container image definition
â”‚
â”œâ”€â”€ mlops/                         # MLOps & Experiment Tracking
â”‚   â”œâ”€â”€ mlflow_tracking.py         # MLflow experiment logging
â”‚   â”œâ”€â”€ model_registry.py          # Model versioning & registry
â”‚   â””â”€â”€ experiment_runner.py       # Automated experiment execution
â”‚
â”œâ”€â”€ deployment/                    # Model Deployment
â”‚   â”œâ”€â”€ api.py                     # FastAPI REST service
â”‚   â”œâ”€â”€ predictor.py               # Prediction logic
â”‚   â””â”€â”€ config.py                  # API configuration
â”‚
â”œâ”€â”€ dashboard/                     # Interactive Dashboard
â”‚   â”œâ”€â”€ app.py                     # Streamlit application
â”‚   â”œâ”€â”€ components/                # UI components
â”‚   â”‚   â”œâ”€â”€ prediction_ui.py       # Prediction interface
â”‚   â”‚   â”œâ”€â”€ monitoring_ui.py       # Monitoring dashboard
â”‚   â”‚   â””â”€â”€ visualizations.py      # Chart components
â”‚   â””â”€â”€ utils.py                   # Helper functions
â”‚
â”œâ”€â”€ monitoring/                    # Model Monitoring
â”‚   â”œâ”€â”€ performance_tracker.py     # Performance metrics tracking
â”‚   â”œâ”€â”€ drift_detector.py          # Data/concept drift detection
â”‚   â”œâ”€â”€ alerting.py                # Alert system
â”‚   â””â”€â”€ retraining_scheduler.py    # Automated retraining logic
â”‚
â””â”€â”€ models/                        # Saved Models
    â”œâ”€â”€ best_model.pkl             # Production model
    â”œâ”€â”€ model_metadata.json        # Model information
    â””â”€â”€ feature_config.json        # Feature configurations
```

---

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install dependencies
pip install -r requirements.txt
```

### 2. Train and Log Model with MLflow

```bash
# Run MLflow experiment
python mlops/experiment_runner.py

# Start MLflow UI
mlflow ui --port 5000
# Visit: http://localhost:5000
```

### 3. Deploy API

```bash
# Start FastAPI server
uvicorn deployment.api:app --reload --port 8000

# API Documentation: http://localhost:8000/docs
```

### 4. Launch Dashboard

```bash
# Start Streamlit dashboard
streamlit run dashboard/app.py

# Visit: http://localhost:8501
```

### 5. Docker Deployment

```bash
# Build and run all services
docker-compose up --build

# Services:
# - API: http://localhost:8000
# - Dashboard: http://localhost:8501
# - MLflow: http://localhost:5000
```

---

## ğŸ”§ Components

### 1. MLOps & Experiment Tracking

**Purpose:** Track experiments, versions, and model lineage

**Features:**

- Automated experiment logging with MLflow
- Parameter, metric, and artifact tracking
- Model versioning and registry
- Reproducible experiment runs
- Model comparison tools

**Usage:**

```python
from mlops.mlflow_tracking import MLflowTracker

tracker = MLflowTracker(experiment_name="sales_forecasting")
tracker.log_model_training(model, metrics, params, features)
```

### 2. Model Deployment API

**Purpose:** Serve predictions via REST API

**Endpoints:**

- `POST /predict` - Single prediction
- `POST /predict/batch` - Batch predictions
- `GET /health` - Service health check
- `GET /model/info` - Model metadata

**Example Request:**

```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "Store": 1,
    "Dept": 1,
    "Date": "2023-11-22",
    "IsHoliday": false,
    "Temperature": 42.31,
    "Fuel_Price": 2.572,
    "Type": "A"
  }'
```

### 3. Interactive Dashboard

**Purpose:** User-friendly interface for predictions and monitoring

**Features:**

- Real-time sales predictions
- Historical performance analysis
- Model metrics visualization
- Feature importance display
- Data drift monitoring
- Interactive charts and filters

### 4. Model Monitoring

**Purpose:** Track model performance and detect issues

**Features:**

- Performance metric tracking over time
- Data drift detection (feature distributions)
- Concept drift detection (prediction accuracy)
- Automated alerting for performance degradation
- Retraining triggers and scheduling

**Metrics Monitored:**

- MAE, RMSE, RÂ² score
- Prediction distribution shifts
- Feature value distributions
- API response times
- Error rates

---

## ğŸ“Š Monitoring & Alerts

### Performance Degradation Alerts

System triggers alerts when:

- RÂ² score drops below 0.90 (from 0.9996 baseline)
- MAE increases above $500 (from $106.77 baseline)
- Feature drift detected (KS test p-value < 0.05)
- Prediction anomalies (outliers > 3 std dev)

### Retraining Strategy

**Automatic Retraining Triggers:**

1. **Scheduled:** Monthly retraining with new data
2. **Performance-Based:** When RÂ² < 0.90 for 7 days
3. **Drift-Based:** When feature drift detected in 3+ features
4. **Manual:** On-demand via dashboard or API

---

## ğŸ³ Docker Deployment

### Services

1. **API Service** (Port 8000)

   - FastAPI application
   - Model inference
   - Health checks

2. **Dashboard** (Port 8501)

   - Streamlit UI
   - Interactive visualizations
   - Monitoring interface

3. **MLflow Server** (Port 5000)
   - Experiment tracking
   - Model registry
   - Artifact storage

### Commands

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down

# Rebuild
docker-compose up --build
```

---

## ğŸ“ˆ Model Performance (Production)

| Metric             | Value         | Status       |
| ------------------ | ------------- | ------------ |
| **RÂ² Score**       | 0.9996        | âœ… Excellent |
| **MAE**            | $106.77       | âœ… Excellent |
| **RMSE**           | $144.53       | âœ… Excellent |
| **Model Type**     | Random Forest | -            |
| **Features**       | 44 engineered | -            |
| **Training Time**  | ~2 minutes    | -            |
| **Inference Time** | <10ms         | âœ… Fast      |

---

## ğŸ” Security Considerations

- API authentication (API keys)
- Input validation and sanitization
- Rate limiting
- HTTPS in production
- Environment variable management
- Secret management (AWS Secrets Manager, Azure Key Vault)

---

## ğŸ“ Deliverables

### âœ… Completed

1. **Deployed Model** - FastAPI REST API serving predictions
2. **MLOps Pipeline** - MLflow experiment tracking and model registry
3. **Interactive Dashboard** - Streamlit application for users
4. **Monitoring System** - Performance tracking and drift detection
5. **Docker Containerization** - Production-ready containers
6. **MLOps Report** - Comprehensive documentation

---

## ğŸ”® Next Steps (Stage 5)

1. Final project documentation
2. Business impact analysis
3. Stakeholder presentation
4. Future improvements roadmap

---

## ğŸ‘¥ Support

For questions or issues:

- Review documentation in each component directory
- Check MLflow UI for experiment history
- Review API logs for debugging
- Monitor dashboard alerts

---

**Status:** âœ… Milestone 4 Complete - Production Ready
