"""
MILESTONE 1 - Step 1.3.4: Normalize Numerical Features
======================================================
Task 3: Preprocessing and Feature Engineering (Final Step)

Normalize numerical features to standardize ranges for ML models.
Using manual Z-score normalization (no sklearn dependency).

Author: Data Science Team
Date: October 23, 2025
"""

import pandas as pd
import numpy as np
import os
import json

print("="*80)
print("STEP 1.3.4: NORMALIZE NUMERICAL FEATURES")
print("="*80)

# Load encoded data
print("\n[1] Loading Encoded Data...")
print("-" * 80)

train = pd.read_csv('processed_data/Stage1.3.3/train_encoded.csv')
test = pd.read_csv('processed_data/Stage1.3.3/test_encoded.csv')

print(f"‚úì Train: {train.shape[0]:,} rows √ó {train.shape[1]} columns")
print(f"‚úì Test:  {test.shape[0]:,} rows √ó {test.shape[1]} columns")

# ============================================================================
# PART 1: IDENTIFY FEATURES TO NORMALIZE
# ============================================================================

print("\n" + "="*80)
print("[2] Identifying Features to Normalize")
print("="*80)

# Features that need normalization (continuous, different scales)
continuous_features = [
    # Store attributes
    'Size',
    # External factors
    'Temperature',
    'Fuel_Price',
    'CPI',
    'Unemployment',
    # Promotional markdowns
    'MarkDown1',
    'MarkDown2',
    'MarkDown3',
    'MarkDown4',
    'MarkDown5',
    # Lag features
    'Sales_Lag1',
    'Sales_Lag2',
    'Sales_Lag4',
    'Sales_Rolling_Mean_4',
    'Sales_Rolling_Mean_8',
    'Sales_Rolling_Std_4',
    'Sales_Momentum'
]

print(f"\nüìä Features to Normalize ({len(continuous_features)}):")
print("-" * 80)

for i, feature in enumerate(continuous_features, 1):
    if feature in train.columns:
        train_min = train[feature].min()
        train_max = train[feature].max()
        train_mean = train[feature].mean()
        train_std = train[feature].std()
        print(f"   {i:2d}. {feature:25s} | Range: [{train_min:>12,.2f}, {train_max:>12,.2f}] | Mean: {train_mean:>10,.2f}")

# Features NOT normalized (already scaled or categorical)
skip_features = [
    'Store', 'Dept', 'Date', 'Weekly_Sales',  # IDs and target
    'Year', 'Month', 'Day', 'Quarter', 'DayOfWeek', 'WeekOfYear',  # Time (small range)
    'IsHoliday', 'Has_MarkDown1', 'Has_MarkDown2', 'Has_MarkDown3', 'Has_MarkDown4', 'Has_MarkDown5',  # Binary
    'Is_Weekend', 'Is_Month_Start', 'Is_Month_End', 'Is_Quarter_Start', 'Is_Quarter_End', 'Is_Year_Start', 'Is_Year_End',  # Binary
    'Month_Sin', 'Month_Cos', 'Week_Sin', 'Week_Cos', 'DayOfWeek_Sin', 'DayOfWeek_Cos',  # Already normalized [-1,1]
    'Type_A', 'Type_B', 'Type_C'  # Binary encoded
]

print(f"\n‚úì Features NOT normalized: {len(skip_features)}")
print("   (IDs, target, binary features, cyclical features)")

# ============================================================================
# PART 2: CALCULATE STATISTICS FROM TRAINING DATA
# ============================================================================

print("\n" + "="*80)
print("[3] Calculating Normalization Parameters from Training Data")
print("="*80)

print("\nüî¨ Z-Score Normalization Formula: z = (x - Œº) / œÉ")
print("   - Œº (mu): Mean of training data")
print("   - œÉ (sigma): Standard deviation of training data")
print("   - Result: mean‚âà0, std‚âà1")

# Calculate mean and std from training data only
normalization_params = {}

print(f"\nüìä Computing statistics for {len(continuous_features)} features...")
print("-" * 80)

for feature in continuous_features:
    if feature in train.columns:
        mean_val = train[feature].mean()
        std_val = train[feature].std()
        
        # Store parameters
        normalization_params[feature] = {
            'mean': mean_val,
            'std': std_val
        }
        
        print(f"   {feature:25s} | Œº = {mean_val:>12,.4f} | œÉ = {std_val:>10,.4f}")

print(f"\n‚úì Calculated parameters for {len(normalization_params)} features")

# ============================================================================
# PART 3: APPLY Z-SCORE NORMALIZATION
# ============================================================================

print("\n" + "="*80)
print("[4] Applying Z-Score Normalization")
print("="*80)

print(f"\nüîß Normalizing TRAINING data...")
for feature in continuous_features:
    if feature in train.columns:
        mean_val = normalization_params[feature]['mean']
        std_val = normalization_params[feature]['std']
        
        # Avoid division by zero
        if std_val > 0:
            train[feature] = (train[feature] - mean_val) / std_val
        else:
            print(f"   ‚ö†Ô∏è Warning: {feature} has std=0, setting to 0")
            train[feature] = 0

print(f"   ‚úì Normalized {len(continuous_features)} features in training data")

print(f"\nüîß Normalizing TEST data with TRAINING parameters...")
for feature in continuous_features:
    if feature in test.columns:
        mean_val = normalization_params[feature]['mean']
        std_val = normalization_params[feature]['std']
        
        # Use same parameters from training data
        if std_val > 0:
            test[feature] = (test[feature] - mean_val) / std_val
        else:
            test[feature] = 0

print(f"   ‚úì Normalized {len(continuous_features)} features in test data")

print("\nüí° Critical: Same parameters (Œº, œÉ) from training used for test data")
print("   This prevents data leakage and ensures consistency")

# ============================================================================
# PART 4: VERIFY NORMALIZATION
# ============================================================================

print("\n" + "="*80)
print("[5] Verifying Normalization")
print("="*80)

print(f"\nüìä Normalized Statistics (Training Data):")
print("-" * 80)
print(f"   {'Feature':<25} | {'Mean':>10} | {'Std':>10} | {'Min':>10} | {'Max':>10}")
print("-" * 80)

verification_features = ['Size', 'Temperature', 'Fuel_Price', 'CPI', 'Sales_Lag1', 'Sales_Rolling_Mean_4']
for feature in verification_features:
    if feature in train.columns:
        mean_val = train[feature].mean()
        std_val = train[feature].std()
        min_val = train[feature].min()
        max_val = train[feature].max()
        print(f"   {feature:<25} | {mean_val:>10.4f} | {std_val:>10.4f} | {min_val:>10.2f} | {max_val:>10.2f}")

print("\n‚úÖ Expected: Mean ‚âà 0.0000, Std ‚âà 1.0000")
print("   (Small deviations are normal due to floating point precision)")

# ============================================================================
# PART 5: SAVE NORMALIZATION PARAMETERS & DATA
# ============================================================================

print("\n" + "="*80)
print("[6] Saving Normalization Parameters and Data")
print("="*80)

# Create output directory
output_dir = 'processed_data/Stage1.3.4_Final'
os.makedirs(output_dir, exist_ok=True)

# Save normalization parameters as JSON
params_path = os.path.join(output_dir, 'normalization_params.json')
with open(params_path, 'w') as f:
    json.dump(normalization_params, f, indent=2)

print(f"\nüíæ Saved Normalization Parameters:")
print(f"   File: {params_path}")
print(f"   ‚ö†Ô∏è IMPORTANT: Use these parameters for ALL future predictions!")
print(f"   Contains mean (Œº) and std (œÉ) for {len(normalization_params)} features")

# Save normalized datasets
train_output = os.path.join(output_dir, 'train_final.csv')
test_output = os.path.join(output_dir, 'test_final.csv')

train.to_csv(train_output, index=False)
test.to_csv(test_output, index=False)

train_size = os.path.getsize(train_output) / (1024 * 1024)
test_size = os.path.getsize(test_output) / (1024 * 1024)

print(f"\nüíæ Saved Final Training Data:")
print(f"   File: {train_output}")
print(f"   Size: {train_size:.2f} MB")
print(f"   Rows: {len(train):,}")
print(f"   Columns: {len(train.columns)}")

print(f"\nüíæ Saved Final Test Data:")
print(f"   File: {test_output}")
print(f"   Size: {test_size:.2f} MB")
print(f"   Rows: {len(test):,}")
print(f"   Columns: {len(test.columns)}")

# ============================================================================
# PART 6: FINAL FEATURE SUMMARY
# ============================================================================

print("\n" + "="*80)
print("[7] Final Feature Engineering Summary")
print("="*80)

print(f"\nüìä Complete Feature Breakdown:")
print("-" * 80)

feature_groups = {
    'Identifiers': ['Store', 'Dept', 'Date'],
    'Target Variable': ['Weekly_Sales'],
    'Store Attributes (normalized)': ['Size'],
    'Store Type (one-hot)': ['Type_A', 'Type_B', 'Type_C'],
    'External Factors (normalized)': ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment'],
    'Promotional $ (normalized)': ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'],
    'Promotional Indicators': ['Has_MarkDown1', 'Has_MarkDown2', 'Has_MarkDown3', 'Has_MarkDown4', 'Has_MarkDown5'],
    'Holiday': ['IsHoliday'],
    'Time Components': ['Year', 'Month', 'Day', 'Quarter', 'DayOfWeek', 'WeekOfYear'],
    'Time Binary': ['Is_Weekend', 'Is_Month_Start', 'Is_Month_End', 'Is_Quarter_Start', 'Is_Quarter_End', 'Is_Year_Start', 'Is_Year_End'],
    'Time Cyclical': ['Month_Sin', 'Month_Cos', 'Week_Sin', 'Week_Cos', 'DayOfWeek_Sin', 'DayOfWeek_Cos'],
    'Lag Features (normalized)': ['Sales_Lag1', 'Sales_Lag2', 'Sales_Lag4'],
    'Rolling Stats (normalized)': ['Sales_Rolling_Mean_4', 'Sales_Rolling_Mean_8', 'Sales_Rolling_Std_4'],
    'Momentum (normalized)': ['Sales_Momentum']
}

total_features = 0
for group, features in feature_groups.items():
    count = len([f for f in features if f in train.columns])
    total_features += count
    print(f"\n   {group}:")
    print(f"      Count: {count}")
    if count <= 10:
        existing_features = [f for f in features if f in train.columns]
        if existing_features:
            print(f"      Features: {', '.join(existing_features)}")

print(f"\n   {'='*70}")
print(f"   TOTAL FEATURES (Train): {len(train.columns)}")
print(f"   TOTAL FEATURES (Test):  {len(test.columns)}")
print(f"   Modeling Features: {len(train.columns) - 3}  (excluding Store, Dept, Date)")

# ============================================================================
# SUMMARY
# ============================================================================

print("\n" + "="*80)
print("‚úÖ STEP 1.3.4 COMPLETED SUCCESSFULLY!")
print("="*80)

print(f"""
üìä Normalization Summary:

   Method: Z-Score Normalization (Manual Implementation)
   Formula: z = (x - Œº) / œÉ
   
   Features Normalized: {len(continuous_features)}
   ‚îú‚îÄ Store Attributes: 1 (Size)
   ‚îú‚îÄ External Factors: 4 (Temperature, Fuel_Price, CPI, Unemployment)
   ‚îú‚îÄ Promotional $: 5 (MarkDown1-5)
   ‚îî‚îÄ Lag/Rolling: 7 (Sales_Lag1-4, Rolling stats, Momentum)

   Result: All normalized features have mean‚âà0, std‚âà1

üìÅ Final Output Files:
   ‚îú‚îÄ {train_output}
   ‚îú‚îÄ {test_output}
   ‚îî‚îÄ {params_path}

üéØ Dataset Ready for Modeling:
   ‚úÖ 49 features total (48 for test - no Weekly_Sales)
   ‚úÖ All numerical features normalized
   ‚úÖ Categorical variables encoded (Type_A, Type_B, Type_C)
   ‚úÖ Time features created (20 features)
   ‚úÖ Lag features created (7 features)
   ‚úÖ Train-test consistency maintained
   ‚úÖ No missing values
   ‚úÖ Normalization parameters saved for production

üîú Next Steps:
   - Complete EDA Report (Step 1.4)
   - Milestone 2: Model Development
   - Build baseline models
   - Train Random Forest, XGBoost, LSTM, etc.
   - Evaluate and compare models
""")

print("="*80)
print("üéâ FEATURE ENGINEERING COMPLETE!")
print("="*80)
